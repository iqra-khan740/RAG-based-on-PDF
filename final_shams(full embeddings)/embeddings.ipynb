{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "!pip list",
   "id": "963b183750330182",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:32:36.894200Z",
     "start_time": "2026-01-28T14:32:27.074512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os, pickle\n",
    "\n",
    "PDF_PATH = \"BOOK.pdf\"\n",
    "SAVE_DIR = \"../faiss_layer1\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Build FAISS\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Save FAISS index\n",
    "vectorstore.save_local(SAVE_DIR)\n",
    "\n",
    "print(f\"âœ… Layer 1 built with {len(chunks)} chunks\")"
   ],
   "id": "d25686ba0900667b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LS\\AppData\\Local\\Temp\\ipykernel_3616\\1851037292.py:26: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 177.32it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Layer 1 built with 23 chunks\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T14:59:11.704654Z",
     "start_time": "2026-01-28T14:43:17.706259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "import os, pickle, faiss\n",
    "import numpy as np\n",
    "\n",
    "LAYER1_DIR = \"../faiss_layer1\"\n",
    "LAYER2_DIR = \"../faiss_layer2\"\n",
    "os.makedirs(LAYER2_DIR, exist_ok=True)\n",
    "\n",
    "# Load Layer 1\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    LAYER1_DIR,\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "raw_docstore = vectorstore.docstore._dict\n",
    "\n",
    "# LLM\n",
    "llm = Ollama(model=\"gemma3:4b\")\n",
    "\n",
    "summary_texts = []\n",
    "\n",
    "print(f\"Total chunks: {len(raw_docstore)}\\n\")\n",
    "\n",
    "for i, (doc_id, doc) in enumerate(raw_docstore.items(), start=1):\n",
    "    try:\n",
    "        summary = llm.invoke(\n",
    "            f\"Summarize this text factually without adding new information:\\n{doc.page_content}\"\n",
    "        )\n",
    "        summary_texts.append({\n",
    "            \"id\": doc_id,\n",
    "            \"text\": summary\n",
    "        })\n",
    "        print(f\"âœ… {i} summarized\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {i} failed: {e}\")\n",
    "\n",
    "# Embed summaries\n",
    "summary_vectors = embedding_model.embed_documents(\n",
    "    [s[\"text\"] for s in summary_texts]\n",
    ")\n",
    "\n",
    "dimension = len(summary_vectors[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(summary_vectors).astype(\"float32\"))\n",
    "\n",
    "# Save metadata\n",
    "docstore = {s[\"id\"]: s[\"text\"] for s in summary_texts}\n",
    "index_to_docstore_id = {i: s[\"id\"] for i, s in enumerate(summary_texts)}\n",
    "\n",
    "with open(f\"{LAYER2_DIR}/index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\"docstore\": docstore, \"mapping\": index_to_docstore_id},\n",
    "        f\n",
    "    )\n",
    "\n",
    "faiss.write_index(index, f\"{LAYER2_DIR}/index.faiss\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Layer 2 built successfully\")\n"
   ],
   "id": "74c3cb194317d00c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 235.50it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "C:\\Users\\LS\\AppData\\Local\\Temp\\ipykernel_3616\\2592347277.py:25: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3:4b\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 23\n",
      "\n",
      "âœ… 1 summarized\n",
      "âœ… 2 summarized\n",
      "âœ… 3 summarized\n",
      "âœ… 4 summarized\n",
      "âœ… 5 summarized\n",
      "âœ… 6 summarized\n",
      "âœ… 7 summarized\n",
      "âœ… 8 summarized\n",
      "âœ… 9 summarized\n",
      "âœ… 10 summarized\n",
      "âœ… 11 summarized\n",
      "âœ… 12 summarized\n",
      "âœ… 13 summarized\n",
      "âœ… 14 summarized\n",
      "âœ… 15 summarized\n",
      "âœ… 16 summarized\n",
      "âœ… 17 summarized\n",
      "âœ… 18 summarized\n",
      "âœ… 19 summarized\n",
      "âœ… 20 summarized\n",
      "âœ… 21 summarized\n",
      "âœ… 22 summarized\n",
      "âœ… 23 summarized\n",
      "\n",
      "ðŸŽ‰ Layer 2 built successfully\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T15:06:27.308625Z",
     "start_time": "2026-01-28T15:06:12.029062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle, faiss\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load embeddings + LLM\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "llm = Ollama(model=\"gemma3:4b\")\n",
    "\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(r\"C:/Users/LS/RAG-based-on-PDF/faiss_layer2\"))\n",
    "\n",
    "FAISS_INDEX_PATH = os.path.join(\n",
    "    BASE_DIR, \"faiss_layer2\", \"index.faiss\"\n",
    ")\n",
    "\n",
    "FAISS_META_PATH = os.path.join(\n",
    "    BASE_DIR, \"faiss_layer2\", \"index.pkl\"\n",
    ")\n",
    "\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "\n",
    "with open(FAISS_META_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\n",
    "docstore = data[\"docstore\"]\n",
    "mapping = data[\"mapping\"]\n",
    "\n",
    "def ask(question, top_k=3):\n",
    "    q_vec = embedding_model.embed_query(question)\n",
    "    distances, indices = index.search(\n",
    "        np.array([q_vec]).astype(\"float32\"),\n",
    "        top_k\n",
    "    )\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        docstore[mapping[i]] for i in indices[0]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Answer the question ONLY using the context below.\n",
    "If the answer is not present, say \"Not found in the document\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "# Example\n",
    "print(ask(\"What is this documentation is all about?\"))"
   ],
   "id": "3b35da814c99b22e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 190.60it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VS Code features, including configuring settings for specific folders, defining debug configurations, and exploring workspaces.\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
